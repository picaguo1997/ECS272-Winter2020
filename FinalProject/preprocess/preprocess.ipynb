{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fox.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization methods: \"Simple split\", nltk, and spacy.\n",
    "Using split now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yhkuo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenization methods\n",
    "# 1. split\n",
    "# Use this one. 2 and 3 are older, update needed.\n",
    "\n",
    "import string\n",
    "def tokenize_split(article):\n",
    "    punct = set(string.punctuation)\n",
    "    collection = article.split()\n",
    "    token = [ ''.join( c for c in w if c not in punct ) for w in collection ]\n",
    "    token = [ w.lower() for w in token ]\n",
    "    return token\n",
    "    \n",
    "# 2. nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "def tokenize_nltk(article):\n",
    "    \n",
    "    collection = []\n",
    "    for line in article:\n",
    "        collection.extend(word_tokenize(line))\n",
    "    token = [''.join(c for c in s if c not in punct) for s in collection]\n",
    "    return token\n",
    "\n",
    "# 3. spacy\n",
    "\n",
    "import spacy\n",
    "def tokenize_spacy(article):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    collection = []\n",
    "    for line in article:\n",
    "        doc = nlp(line)\n",
    "        for token in doc:\n",
    "            collection.append(token.text)\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yhkuo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def removeStopWords(token):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    return [ c for c in token if c not in stopWords ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization\n",
    "To preserve linguistic meaning, use lemmatization instead of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yhkuo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(concisedToken):\n",
    "    return [ WordNetLemmatizer().lemmatize(c) for c in concisedToken ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(concisedToken):\n",
    "    return [ PorterStemmer().stem(c) for c in concisedToken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article):\n",
    "    token = tokenize_split(article)\n",
    "    concisedToken = removeStopWords(token)\n",
    "    stems = [ c for c in stemming(concisedToken) if c != '']\n",
    "    lemmas = [ c for c in lemmatize(concisedToken) if c != '']\n",
    "    return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Israeli from Diamond Princess cruise ship tests positive for coronavirus\n"
     ]
    }
   ],
   "source": [
    "content = data[30]['content']\n",
    "headline = data[30]['headline']\n",
    "print(headline)\n",
    "\n",
    "contentLemma = []\n",
    "contentStems = []\n",
    "for line in content:\n",
    "    stems, lemmas = preprocess(line)\n",
    "    contentStems.append(stems)\n",
    "    contentLemma.append(lemmas)\n",
    "headStems, headLemma = preprocess(headline)\n",
    "\n",
    "contentLemma = [ sen for sen in contentLemma if sen ]\n",
    "contentStems = [ sen for sen in contentStems if sen ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFrequencyMatrix(contentStems):\n",
    "    frequency_matrix = {}\n",
    "    for i in range(len(contentStems)):\n",
    "        sen = contentStems[i]\n",
    "        frequency_table = {}\n",
    "        for word in sen:\n",
    "            if word in frequency_table:\n",
    "                frequency_table[word] += 1\n",
    "            else:\n",
    "                frequency_table[word] = 1\n",
    "        frequency_matrix[i] = frequency_table\n",
    "        \n",
    "    return frequency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTFMatrix(frequency_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for i, table in frequency_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        totalCount = len(table)\n",
    "        for word, count in table.items():\n",
    "            tf_table[word] = count / totalCount\n",
    "\n",
    "        tf_matrix[i] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def createPerWordTable(frequency_matrix):\n",
    "    word_table = {}\n",
    "\n",
    "    for i, table in frequency_matrix.items():\n",
    "        for word, count in table.items():\n",
    "            if word in word_table:\n",
    "                word_table[word] += 1\n",
    "            else:\n",
    "                word_table[word] = 1\n",
    "                \n",
    "    return word_table\n",
    "\n",
    "def createIDFMatrix(frequency_matrix, totalDoc):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    word_table = createPerWordTable(frequency_matrix)\n",
    "    \n",
    "    for i, table in frequency_matrix.items():\n",
    "        idf_table = {}\n",
    "        \n",
    "        for word in table.keys():\n",
    "            idf_table[word] = math.log10(totalDoc / float(word_table[word]))\n",
    "\n",
    "        idf_matrix[i] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTF_IDFMatrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    for (i1, table1), (i2, table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "        tf_idf_table = {}\n",
    "        for (word1, value1), (word2, value2) in zip(table1.items(),table2.items()):\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[i1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def scoring(tf_matrix, idf_matrix):\n",
    "\n",
    "    tf_idf_matrix = createTF_IDFMatrix(tf_matrix, idf_matrix)\n",
    "    \n",
    "    sentenceValue = {}\n",
    "\n",
    "    for i, table in tf_idf_matrix.items():\n",
    "        totalScore = 0\n",
    "\n",
    "        wordCount = len(table)\n",
    "        for word, score in table.items():\n",
    "            totalScore += score\n",
    "\n",
    "        sentenceValue[i] = totalScore / wordCount\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findThreshold(sentenceValue):\n",
    "    summation = 0\n",
    "    for entry in sentenceValue:\n",
    "        summation += sentenceValue[entry]\n",
    "\n",
    "    average = (summation / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSummary(content, contentStems):\n",
    "    summary = ''\n",
    "    f_matrix = createFrequencyMatrix(contentStems)\n",
    "    tf_matrix = createTFMatrix(f_matrix)\n",
    "    idf_matrix = createIDFMatrix(f_matrix, len(contentStems))\n",
    "    scores = scoring(tf_matrix, idf_matrix)\n",
    "    threshold = findThreshold(scores)\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        if i in scores and scores[i] >= 1.5*(threshold):\n",
    "            summary += ' ' + content[i]\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Israeli from Diamond Princess cruise ship tests positive for coronavirus\n",
      " Officials stressed that the patient did not contract the virus in Israel.\n"
     ]
    }
   ],
   "source": [
    "print(headline)\n",
    "print(generateSummary(content, contentStems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Israel’s Health Ministry confirmed Friday the first case of an Israeli citizen having contracted covid-19 while aboard the Diamond Princess cruise ship, docked in a port in Japan. The female patient is under supervision and in isolation, the ministry said, according to Israeli media.', 'Officials stressed that the patient did not contract the virus in Israel.', 'Eleven Israeli citizens were among the more than 3,000 passengers and crew quarantined on the cruise liner after a coronavirus outbreak on board. In total, 634 of the ship’s occupants have tested positive for the virus and two have died of covid-19, according to Japanese health authorities.', 'The 11 Israelis were flown out of Japan and sent directly Friday into isolation at Sheba Tel Hashomer Hospital, where they will remain for a quarantine period.', 'In an effort to prevent the entry of the virus into Israel, Israel’s government on Monday announced a temporary travel ban on all foreign nationals who in the past 14 days had traveled to Thailand, Singapore, Hong Kong and Macao.', 'Thailand has asked Israel to reconsider including it on the ban, which affects about 25,000 Thai workers employed largely in agriculture in Israel.']\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "def sentimentAnalysis(headline):\n",
    "    return SIA().polarity_scores(headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "from collections import OrderedDict\n",
    "\n",
    "processed_data = {}\n",
    "for news in data:\n",
    "    processed_news = {}\n",
    "    content = news['content']\n",
    "    headline = news['headline']\n",
    "    \n",
    "    contentLemma = []\n",
    "    contentStems = []\n",
    "    for line in content:\n",
    "        stems, lemmas = preprocess(line)\n",
    "        if stems and lemmas:\n",
    "            contentStems.append(stems)\n",
    "            contentLemma.append(lemmas)\n",
    "        else:\n",
    "            content.remove(line)\n",
    "    \n",
    "    headStems, headLemma = preprocess(headline)\n",
    "    summary = generateSummary(content, contentStems)\n",
    "    sentiment = sentimentAnalysis(headline)\n",
    "    \n",
    "    processed_news['headline'] = news['headline']\n",
    "    processed_news['headline-sentiment'] = sentiment\n",
    "    processed_news['headline-lemmas'] = headLemma\n",
    "    processed_news['content'] = news['content']\n",
    "    processed_news['content-summary'] = summary\n",
    "    processed_news['content-lemmas'] = contentLemma\n",
    "    processed_news['journal'] = news['journal']\n",
    "    processed_news['url'] = news['url']\n",
    "    \n",
    "    date = news['time-stamp'][:10]\n",
    "    if date in processed_data:\n",
    "        processed_data[date].append(processed_news)\n",
    "    else:\n",
    "        processed_data[date] = [processed_news]\n",
    "\n",
    "result = dict(OrderedDict(sorted(processed_data.items(), key=lambda t: t[0])))\n",
    "with open('processed_fox.json', 'w') as file:\n",
    "    json.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
