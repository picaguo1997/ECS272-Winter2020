{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38264bit8b6ff05588bd409bb1bc77d8893430fc",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions, EmotionOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator('ZtAL7sIQ58MB8UrjOeYR_fvko36zVObg57OQ8y3FftDn')\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2019-07-12',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "natural_language_understanding.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/90eb04a6-053b-4fdd-a09a-2005d9fe421b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a json of one newssource and calls sentiment and emotion analysis for each article and writes them directly to the given json file\n",
    "\n",
    "def analyze_newssource(json):\n",
    "    no_articles = 0\n",
    "\n",
    "    for date in json:\n",
    "        for article in json[date]:\n",
    "            text = str(article['content'])\n",
    "            sentiment, sadness, joy, fear, disgust, anger = analyze_text(str(text))\n",
    "\n",
    "            article['watson_analysis'] = {\n",
    "                'sentiment' : sentiment,\n",
    "                'sadness' : sadness,\n",
    "                'joy' : joy,\n",
    "                'fear' : fear,\n",
    "                'disgust' : disgust,\n",
    "                'anger' : anger,\n",
    "            }\n",
    "            if(no_articles % 10 == 0):\n",
    "                print('Analyzing article ' + str(no_articles) + ' to ' + str(no_articles + 10))\n",
    "            no_articles += 1\n",
    "\n",
    "    print('Analysis done.')\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a text, requests analysis and returns sentiment and emotion results\n",
    "\n",
    "def analyze_text(text):\n",
    "    response = natural_language_understanding.analyze(\n",
    "        text=text,\n",
    "        features=Features(sentiment=SentimentOptions(), emotion=EmotionOptions())\n",
    "    ).get_result()\n",
    "\n",
    "    try:\n",
    "        sentiment = response['sentiment']['document']['score']\n",
    "    except KeyError:\n",
    "        sentiment = None\n",
    "        print('No value for sentiment.')\n",
    "\n",
    "    try:\n",
    "        sadness = response['emotion']['document']['emotion']['sadness']\n",
    "        joy = response['emotion']['document']['emotion']['joy']\n",
    "        fear = response['emotion']['document']['emotion']['fear']\n",
    "        disgust = response['emotion']['document']['emotion']['disgust']\n",
    "        anger = response['emotion']['document']['emotion']['anger']\n",
    "    except KeyError:\n",
    "        sadness = None\n",
    "        joy = None\n",
    "        fear = None\n",
    "        disgust = None\n",
    "        anger = None\n",
    "        print('No value for emotions.')\n",
    "\n",
    "    return sentiment, sadness, joy, fear, disgust, anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_newssources(newssources):\n",
    "\n",
    "  for newssource in newssources:\n",
    "    try:\n",
    "      # open json\n",
    "      fpath = './processed_' + newssource + '.json'\n",
    "\n",
    "      with open(fpath) as f:\n",
    "        print('Opening ' + fpath)\n",
    "        data = json.load(f)\n",
    "\n",
    "      # append analysis\n",
    "      print('Analyzing newssource ' + newssource)\n",
    "      analyzed_json = analyze_newssource(data)\n",
    "\n",
    "      # write json back\n",
    "      with open(fpath, 'w') as f:\n",
    "        print('Writing ' + fpath)\n",
    "        json.dump(analyzed_json, f)\n",
    "        print('===================================')\n",
    "\n",
    "    except IOError as e:\n",
    "      print('I/O error({0}): {1}'.format(e.errno, e.strerror))\n",
    "    except:\n",
    "      print('Unexpected error:', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Opening ./processed_fox.json\nAnalyzing newssource fox\nAnalyzing\nAnalyzing article 0 to 10\nAnalyzing article 10 to 20\nAnalyzing article 20 to 30\nAnalyzing article 30 to 40\nAnalyzing article 40 to 50\nAnalyzing article 50 to 60\nAnalyzing article 60 to 70\nAnalyzing article 70 to 80\nAnalyzing article 80 to 90\nAnalyzing article 90 to 100\nAnalyzing article 100 to 110\nAnalyzing article 110 to 120\nAnalyzing article 120 to 130\nAnalyzing article 130 to 140\nAnalyzing article 140 to 150\nAnalyzing article 150 to 160\nAnalyzing article 160 to 170\nAnalyzing article 170 to 180\nAnalyzing article 180 to 190\nAnalysis done.\nWriting ./processed_fox.json\n==============================\nOpening ./processed_breitbart.json\nAnalyzing newssource breitbart\nAnalyzing\nAnalyzing article 0 to 10\nAnalyzing article 10 to 20\nAnalyzing article 20 to 30\nAnalyzing article 30 to 40\nAnalyzing article 40 to 50\nAnalyzing article 50 to 60\nAnalyzing article 60 to 70\nAnalyzing article 70 to 80\nAnalyzing article 80 to 90\nAnalyzing article 90 to 100\nAnalyzing article 100 to 110\nAnalyzing article 110 to 120\nAnalyzing article 120 to 130\nAnalyzing article 130 to 140\nAnalyzing article 140 to 150\nAnalyzing article 150 to 160\nAnalyzing article 160 to 170\nAnalyzing article 170 to 180\nAnalyzing article 180 to 190\nAnalyzing article 190 to 200\nAnalyzing article 200 to 210\nAnalyzing article 210 to 220\nAnalyzing article 220 to 230\nAnalyzing article 230 to 240\nAnalyzing article 240 to 250\nAnalyzing article 250 to 260\nAnalyzing article 260 to 270\nAnalysis done.\nWriting ./processed_breitbart.json\n==============================\nOpening ./processed_washington.json\nAnalyzing newssource washington\nAnalyzing\nAnalyzing article 0 to 10\nAnalyzing article 10 to 20\nAnalyzing article 20 to 30\nAnalyzing article 30 to 40\nAnalyzing article 40 to 50\nAnalysis done.\nWriting ./processed_washington.json\n==============================\n"
    }
   ],
   "source": [
    "newssources = ['fox', 'breitbart', 'washington']\n",
    "\n",
    "analyze_newssources(newssources)"
   ]
  }
 ]
}